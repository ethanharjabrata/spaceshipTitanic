import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from torch.nn import Sequential, Linear, ReLU, BCEWithLogitsLoss
from torch.optim import Adam
from torch import from_numpy, sigmoid
import random
def performOneHotEncode(train, encoded_col, encoder):
  '''Remember to fill in any NA values with Unknown prior to doing this'''
  encoded_arrays=encoder.fit_transform(train[[encoded_col]])
  encoded_df=pd.DataFrame(
      encoded_arrays,
      columns=encoder.get_feature_names_out([encoded_col]),
      index=train.index
  )
  return pd.concat([train, encoded_df], axis=1)
def performScaling(train, scaling_col, fittedScaler):
  train[scaling_col]=fittedScaler.transform(train[[scaling_col]])
  return train
def setSeed(seed):
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  
  # Crucial for GPU determinism
  torch.backends.cudnn.deterministic = True
  torch.backends.cudnn.benchmark = False

def main():
  #Cleaning and Preprocessing
  train = pd.read_csv('data/train.csv')
  test = pd.read_csv('data/test.csv')
  category_encoder = OneHotEncoder(sparse_output=False, drop=["Unknown"])
  scaler=StandardScaler()
  ##HOME PLANET
  train['HomePlanet']=train['HomePlanet'].fillna("Unknown")
  train=performOneHotEncode(train, "HomePlanet", category_encoder)

  test['HomePlanet']=test['HomePlanet'].fillna("Unknown")
  test=performOneHotEncode(test, "HomePlanet", category_encoder)

  ##CRYOSLEEP
  # print(train["CryoSleep"].astype(float).hist())
  train['CryoSleep']=train['CryoSleep'].fillna(False)
  train['CryoSleep'] = train['CryoSleep'].astype(int)

  test['CryoSleep']=test['CryoSleep'].fillna(False)
  test['CryoSleep'] = test['CryoSleep'].astype(int)

  ##CABIN
  #step 1: split into deck, num and side
  train["deck"]=train['Cabin'].str.split('/').str[0]
  train["side"]=train['Cabin'].str.split('/').str[2]
  train['deck']=train['deck'].fillna("Unknown")
  train=performOneHotEncode(train, 'deck', category_encoder)
  train['side']=train['side'].fillna("Unknown")
  train=performOneHotEncode(train, 'side', category_encoder)

  test["deck"]=test['Cabin'].str.split('/').str[0]
  test["side"]=test['Cabin'].str.split('/').str[2]
  test['deck']=test['deck'].fillna("Unknown")
  test=performOneHotEncode(test, 'deck', category_encoder)
  test['side']=test['side'].fillna("Unknown")
  test=performOneHotEncode(test, 'side', category_encoder)

  ##DESTINATION
  train['Destination']=train['Destination'].fillna("Unknown")
  train=performOneHotEncode(train, 'Destination', category_encoder)

  test['Destination']=test['Destination'].fillna("Unknown")
  test=performOneHotEncode(test, 'Destination', category_encoder)

  ##AGE
  train['Age']= train['Age'].fillna(0)
  scaler.fit(train[['Age']])
  train=performScaling(train, 'Age', scaler)

  test['Age']= test['Age'].fillna(0)
  test=performScaling(test, 'Age', scaler)


  ##VIP
  train['VIP']=train['VIP'].fillna(False)
  train['VIP'] = train['VIP'].astype(int)

  test['VIP']=test['VIP'].fillna(False)
  test['VIP'] = test['VIP'].astype(int)

  ##LUXURY SERVICES
  services=["RoomService", "FoodCourt","ShoppingMall", "Spa", "VRDeck"]
  train[services]=train[services].fillna(0)
  test[services]=test[services].fillna(0)
  for service in services:
      scaler.fit(train[[service]])
      train=performScaling(train,service, scaler)
      test=performScaling(test,service, scaler)
  #Name is being dropped because I sincerely doubt its relevant
  #room_num, Destination_55 Cancri, deck_T, deck_D, deck_G are also being dropped based on model weights generated by Ridge and Lasso
  to_drop=['Name', 'HomePlanet', 'Cabin', 'deck', 'side', "Destination", "deck_T", "deck_D", "Destination_55 Cancri e", "deck_G"]
  train=train.drop(columns=to_drop)
  test=test.drop(columns=to_drop)
  # print(train.isna().sum())
  y=train['Transported']
  x=train.drop(columns=['Transported','PassengerId'])
  x_train, x_val, y_train, y_val = train_test_split(x,y, test_size=0.2, random_state=100)
  x_test=test.drop(columns=['PassengerId'])
  
  # MLP training
  #Set seed for reproducability
  setSeed(100)
  #In the off chance my GPU has a meltdown before this code is run
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  #Convert to Tensors and then move to GPU
  x_train_tensor=from_numpy(x_train.values)
  #Cast from Double (default) to float so PyTorch doesn't go insane
  x_train_tensor=x_train_tensor.to(device=device).float()
  y_train_tensor=from_numpy(y_train.values)
  #convert to 2D shape (admittedly still a 1D vector) so PyTorch doesn't go insane
  y_train_tensor=y_train_tensor.to(device=device).float().view(-1,1)

  x_val_tensor=from_numpy(x_val.values)
  x_val_tensor=x_val_tensor.to(device=device).float()
  y_val_tensor=from_numpy(y_val.values)
  y_val_tensor=y_val_tensor.to(device=device).float().view(-1,1)

  #2 layer so my comuputer doesn't have a meltdown
  # print(y_train_tensor.size())
  model = Sequential(
    Linear(x_train.shape[1], 50, device=device),
    ReLU(),
    Linear(50, 1, device=device),
  )
  #set reduction to mean so that I don't have to mess with the learning rate
  loss_func=BCEWithLogitsLoss(reduction="mean")
  epochs=200
  learning_rate=1e-2

  #early stopping
  best_loss=float("inf")
  best_model_weights=None
  #allow for 10 increasingly inacurrate weights before stopping
  max_count=10
  count=0
  optimizer= Adam(params=model.parameters(), lr=learning_rate)
  for i in range(epochs):
    #Training
    model.train()
    y_pred=model(x_train_tensor)
    loss=loss_func(y_pred, y_train_tensor)
    # print(f"epoch: {i}, loss: {loss.item():.5f}")
    #Zero gradients to prevent new calculated gradient being added to old gradient from previous epochs
    optimizer.zero_grad()
    loss.backward()
    #Update the model using partial derivates calculated during backward pass
    optimizer.step()

    #Evaluation against validation set
    model.eval()
    with torch.no_grad():
      val_pred=model(x_val_tensor)
      val_loss=loss_func(val_pred, y_val_tensor)
    # print(f"epoch: {i}, train loss: {loss.item():.5f}, val loss: {val_loss.item():.5f}")
    if val_loss<best_loss:
      best_loss=val_loss
      best_model_weights=model.state_dict()
      count=0
    else:
      count+=1
      if count>=max_count:
        print(f"__EARLY STOP, EPOCH {i}__")
        model.load_state_dict(best_model_weights)
        break
      
  #Get final predictions
  with torch.no_grad():
    outputs= model(x_val_tensor)
    #scale from regression model to classification model
    probs=sigmoid(outputs)
    #round up or down to find True/False
    tensor_predictions=(probs>0.5)
    predictions=tensor_predictions.cpu().detach().numpy()
  #Accuracy 80.6% with 36 epochs and lr 0.01
  print(f"Accuracy: {accuracy_score(y_val, predictions)}")

  x_test_tensor=from_numpy(x_test.values)
  x_test_tensor=x_test_tensor.to(device=device).float()
  # Create submission
  with torch.no_grad():
    outputs= model(x_test_tensor)
    #scale from regression model to classification model
    probs=sigmoid(outputs)
    #round up or down to find True/False
    tensor_predictions=(probs>0.5)
    predictions=tensor_predictions.cpu().detach().numpy()
  predictions=pd.DataFrame(predictions)
  submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Transported': predictions[predictions.columns[0]]})
  #True Accuracy: 79.8%
  submission.to_csv('data/MLPSubmission.csv', index=False) 
  
main()